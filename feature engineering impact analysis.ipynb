{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  <h2 style=\"color:#000000\";> Impact of feature engineering on models' performance</h2>\n",
    "\n",
    "<h3 style=\"color:#000000\";> Abdelwahid Benslimane</h3>\n",
    "    <h3 style=\"color:#000000\";> wahid.benslimane@gmail.com</h3>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The purpose of the present experiment is to show that a little work on the data, both in terms of dimensionality and format, can improve the performance of a model, provided that hyper-parameters may also be modified.<br>\n",
    "\n",
    "Also, depending on the model, the optimal data transformation may differ. \n",
    "    \n",
    "The procedure followed was simple and involved using the German Credit dataset available on many sites and which contains both numeric and categorical data. The dataset categorizes 1,000 individuals characterized by 20 variables (target variable not included) as either customers at risk or not at risk for granting credit. Among the individuals, 300 are considered at risk, and the 700 others are considered as not at risk. The dataset has no missing value. It can be downloaded from there: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "I carried out several different transformations:\n",
    "\n",
    "- First, I discretized the real-valued numerical variables (or numerical variables with very many values) so that the dataset contained only categorical variables. Thindataset obtained will be called data0 in the following explanations. It contains 20 variables (target variable not included).\n",
    "    \n",
    "\n",
    "- I created a first binary dataset using the complete disjunctive array (one-hot encoding) obtained from data0. This dataset contains 77 binary variables, corresponding to the sum of the modalities of all the categorical variables in data0. This is the first data set\n",
    "\n",
    "\n",
    "- I performed a multiple component analysis (MCA) on data0 to obtain a dataset of real-valued synthetic variables. The maximum number of factors is 77, but the MCA retained only 76, because the first 76 components contain the total inertia of the data. This is the second dataset. \n",
    "\n",
    "\n",
    "- I selected the first 35 components, which together contain more than 77% of the data inertia. This is the third data set.\n",
    "\n",
    "\n",
    "- I transformed the String variables in data0 into ordinal variables, as the models used only work with numerical values. This fourth dataset therefore contains 20 variables, just like data0.\n",
    "\n",
    "<h3><u>It is important to mention that using ordinal variables in this specific case is not totally relevant as not all the variables have their values that can support an order relation, but it is worth checking the performances doing that.</u></h3>\n",
    "\n",
    "For the demonstration I worked on 2 fairly simple models, the decision tree and SVM with the kernel trick. \n",
    "For the decision tree, a first test was carried out with a complete tree, then a pruned decision tree in order to improve the tree's generalization capacity by limiting its depth.<br>\n",
    "\n",
    "As for the SVM, tests were made with several kernels and several values for each parameter of each kernel.<br>\n",
    "\n",
    "Model optimization was simply performed using a grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> I) Data loading, discretization of numerical variables and creation of the binary dataset</3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prince library (for factor analysis) can be installed from a Jupyther notebook using the folowwing comands:\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns of the complete disjonctive array (one hot encoding):\n",
      "77\n",
      "Number of individuals categorized as customers at risk:\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#data prep\n",
    "\n",
    "#building a DataFrame from the structured data file\n",
    "\n",
    "df = pd.read_table('german.data', delim_whitespace=True, names=('Status of existing checking account', 'Duration (months)', \n",
    "                                                                'Credit history', 'Purpose', 'Credit amount', \\\n",
    "                                                                'Saving account/bounds', 'Present employment since', \\\n",
    "                                                                'Install. rate of disposable income', 'Personal status and sex', \\\n",
    "                                                                'Other debtors / guarantors', 'Present residence since', \\\n",
    "                                                                'Property', 'Age (years)', 'Other installment plans', \\\n",
    "                                                                'Housing', 'Number of existing credits at this bank', \\\n",
    "                                                                'Job', \\\n",
    "                                                                'Number of people being liable to provide maintenance for', \\\n",
    "                                                                'Telephone', 'Foreign worker', 'Customer risk category') )\n",
    "\n",
    "#\"Customer risk category\" is the target variable\n",
    "\n",
    "#convert integer variables with a small number of finite values to string type\n",
    "\n",
    "df[\"Number of people being liable to provide maintenance for\"] = df[\"Number of people being liable to provide maintenance for\"]\\\n",
    ".astype(str)\n",
    "df[\"Install. rate of disposable income\"] = df[\"Install. rate of disposable income\"].astype(str)\n",
    "df[\"Present residence since\"]= df[\"Present residence since\"].astype(str)\n",
    "df[\"Number of existing credits at this bank\"]= df[\"Number of existing credits at this bank\"].astype(str)\n",
    "\n",
    "#transforming other numerical variables into categorical variables \n",
    "#by replacing values by the intervals to which they belong  \n",
    "df[\"Age (years)\"] = pd.qcut(df[\"Age (years)\"], 3, labels=['young', 'medium-aged', 'senior'])\n",
    "df[\"Duration (months)\"] = pd.qcut(df[\"Duration (months)\"], 3, labels=['low', 'medium', 'high'])\n",
    "df[\"Credit amount\"] = pd.qcut(df[\"Credit amount\"], 3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "#extraction of the target variable then conversion of value 2 to 1 and value 1 to 0\n",
    "Y = df[\"Customer risk category\"].copy() -1\n",
    "\n",
    "#extraction of the explanatory variables \n",
    "data0 = df.drop(columns = \"Customer risk category\")\n",
    "\n",
    "#one hot encoding of the explanatory varaiables\n",
    "X_dummy = pd.get_dummies(data0)\n",
    "\n",
    "print(\"Number of columns of the complete disjonctive array (one hot encoding):\")\n",
    "print(np.shape(X_dummy)[1])\n",
    "print(\"Number of individuals categorized as customers at risk:\")\n",
    "print(sum(Y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>II) MCA on data0 and analysis of 40 first components</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eigenvalue</th>\n",
       "      <th>% of variance</th>\n",
       "      <th>% of variance (cumulative)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159</td>\n",
       "      <td>5.58%</td>\n",
       "      <td>5.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113</td>\n",
       "      <td>3.97%</td>\n",
       "      <td>9.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106</td>\n",
       "      <td>3.73%</td>\n",
       "      <td>13.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090</td>\n",
       "      <td>3.14%</td>\n",
       "      <td>16.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081</td>\n",
       "      <td>2.83%</td>\n",
       "      <td>19.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.080</td>\n",
       "      <td>2.80%</td>\n",
       "      <td>22.05%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.075</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>24.67%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.073</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>27.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.070</td>\n",
       "      <td>2.45%</td>\n",
       "      <td>29.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.069</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>32.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.066</td>\n",
       "      <td>2.31%</td>\n",
       "      <td>34.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.065</td>\n",
       "      <td>2.27%</td>\n",
       "      <td>36.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.063</td>\n",
       "      <td>2.21%</td>\n",
       "      <td>38.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.061</td>\n",
       "      <td>2.15%</td>\n",
       "      <td>41.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.060</td>\n",
       "      <td>2.11%</td>\n",
       "      <td>43.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059</td>\n",
       "      <td>2.07%</td>\n",
       "      <td>45.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.058</td>\n",
       "      <td>2.02%</td>\n",
       "      <td>47.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.056</td>\n",
       "      <td>1.97%</td>\n",
       "      <td>49.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.055</td>\n",
       "      <td>1.94%</td>\n",
       "      <td>51.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.054</td>\n",
       "      <td>1.90%</td>\n",
       "      <td>53.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.053</td>\n",
       "      <td>1.85%</td>\n",
       "      <td>54.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.052</td>\n",
       "      <td>1.83%</td>\n",
       "      <td>56.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.052</td>\n",
       "      <td>1.82%</td>\n",
       "      <td>58.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.051</td>\n",
       "      <td>1.78%</td>\n",
       "      <td>60.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.050</td>\n",
       "      <td>1.74%</td>\n",
       "      <td>62.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.049</td>\n",
       "      <td>1.72%</td>\n",
       "      <td>63.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.048</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>65.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.046</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>67.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.045</td>\n",
       "      <td>1.59%</td>\n",
       "      <td>68.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.044</td>\n",
       "      <td>1.56%</td>\n",
       "      <td>70.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.044</td>\n",
       "      <td>1.55%</td>\n",
       "      <td>71.78%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.043</td>\n",
       "      <td>1.52%</td>\n",
       "      <td>73.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.043</td>\n",
       "      <td>1.50%</td>\n",
       "      <td>74.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.041</td>\n",
       "      <td>1.45%</td>\n",
       "      <td>76.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.040</td>\n",
       "      <td>1.41%</td>\n",
       "      <td>77.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.039</td>\n",
       "      <td>1.38%</td>\n",
       "      <td>79.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.039</td>\n",
       "      <td>1.36%</td>\n",
       "      <td>80.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.038</td>\n",
       "      <td>1.33%</td>\n",
       "      <td>81.73%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.037</td>\n",
       "      <td>1.29%</td>\n",
       "      <td>83.02%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.037</td>\n",
       "      <td>1.28%</td>\n",
       "      <td>84.31%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          eigenvalue % of variance % of variance (cumulative)\n",
       "component                                                    \n",
       "0              0.159         5.58%                      5.58%\n",
       "1              0.113         3.97%                      9.55%\n",
       "2              0.106         3.73%                     13.27%\n",
       "3              0.090         3.14%                     16.41%\n",
       "4              0.081         2.83%                     19.24%\n",
       "5              0.080         2.80%                     22.05%\n",
       "6              0.075         2.62%                     24.67%\n",
       "7              0.073         2.58%                     27.24%\n",
       "8              0.070         2.45%                     29.69%\n",
       "9              0.069         2.41%                     32.10%\n",
       "10             0.066         2.31%                     34.42%\n",
       "11             0.065         2.27%                     36.69%\n",
       "12             0.063         2.21%                     38.91%\n",
       "13             0.061         2.15%                     41.06%\n",
       "14             0.060         2.11%                     43.17%\n",
       "15             0.059         2.07%                     45.24%\n",
       "16             0.058         2.02%                     47.26%\n",
       "17             0.056         1.97%                     49.24%\n",
       "18             0.055         1.94%                     51.17%\n",
       "19             0.054         1.90%                     53.07%\n",
       "20             0.053         1.85%                     54.92%\n",
       "21             0.052         1.83%                     56.74%\n",
       "22             0.052         1.82%                     58.56%\n",
       "23             0.051         1.78%                     60.34%\n",
       "24             0.050         1.74%                     62.08%\n",
       "25             0.049         1.72%                     63.81%\n",
       "26             0.048         1.67%                     65.48%\n",
       "27             0.046         1.61%                     67.09%\n",
       "28             0.045         1.59%                     68.68%\n",
       "29             0.044         1.56%                     70.24%\n",
       "30             0.044         1.55%                     71.78%\n",
       "31             0.043         1.52%                     73.30%\n",
       "32             0.043         1.50%                     74.80%\n",
       "33             0.041         1.45%                     76.26%\n",
       "34             0.040         1.41%                     77.66%\n",
       "35             0.039         1.38%                     79.04%\n",
       "36             0.039         1.36%                     80.40%\n",
       "37             0.038         1.33%                     81.73%\n",
       "38             0.037         1.29%                     83.02%\n",
       "39             0.037         1.28%                     84.31%"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "import prince\n",
    "\n",
    "#Multiple Component Analysis to get convert categorial variables \n",
    "#into real numerical variables\n",
    "#the sum of all the values for all the explanatory variables is 77\n",
    "#therefore, the max number of compenant analysis is 77\n",
    "mca = prince.MCA(n_components=77,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "mca = mca.fit(data0)\n",
    "X = mca.transform(data0)\n",
    "\n",
    "#explained inertia by the 40 first compoents \n",
    "mca.eigenvalues_summary[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 35 first components obtained following the application of an MCA carry 77.66 % of the data inertia. Therefore I create both a dataset with the complete synthetic variables and a dataset with the major 35 synthetic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#The 35 first components explain more than 77 % of the inertia, therefore\n",
    "#I create a dataset with the only 35 first related variables \n",
    "#I also create a dataset with the complete variables\n",
    "\n",
    "X_real = X.to_numpy()[:,:35]\n",
    "\n",
    "X_real_complete = X.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> III) Creation of the dataset with ordinal variables from data0</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "#It is important to mention that using ordinal variables in this specific case \n",
    "#is not totally relevant as not all the variables have their values that can support \n",
    "#an order relation, but it is worth checking the performances doing that\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#ordinal encoding of the explanatory variables\n",
    "encoder = OrdinalEncoder()\n",
    "X_ordinal = encoder.fit_transform(data0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><u> Shapes of all the datasets summarized</u>:</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the one-hot encoded (binary) dataset:\n",
      "(1000, 77)\n",
      "Shape of the complete real-valued dataset:\n",
      "(1000, 76)\n",
      "Shape of the real-valued dataset carrying 77.66 % of the inertia:\n",
      "(1000, 35)\n",
      "Shape of dataset with ordinal variables:\n",
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "\n",
    "print(\"Shape of the one-hot encoded (binary) dataset:\")\n",
    "print(np.shape(X_dummy))\n",
    "print(\"Shape of the complete real-valued dataset:\")\n",
    "print(np.shape(X_real_complete))\n",
    "print(\"Shape of the real-valued dataset carrying 77.66 % of the inertia:\")\n",
    "print(np.shape(X_real))\n",
    "print(\"Shape of dataset with ordinal variables:\")\n",
    "print(np.shape(X_ordinal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> IV) Split of the data into train and test datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#train and test split\n",
    "\n",
    "X_dummy_train, X_dummy_test, y_dummy_train, y_dummy_test = train_test_split(X_dummy, Y, train_size=0.7, random_state=0)\n",
    "\n",
    "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(X_real, Y, train_size=0.7, random_state=0)\n",
    "\n",
    "X_real_complete_train, X_real_complete_test, y_real_complete_train, y_real_complete_test = \\\n",
    "train_test_split(X_real_complete, Y, train_size=0.7, random_state=0)\n",
    "\n",
    "X_ordinal_train, X_ordinal_test, y_ordinal_train, y_ordinal_test = \\\n",
    "train_test_split(X_ordinal, Y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>V) Decision tree</h3>\n",
    "    \n",
    "For each type of dataset, I printed the score obatined with a complete tree, the best score obtained once the tree was pruned, the optimal depth of the tree, the confusion matrix and the classification report. \n",
    "\n",
    "<b>It is important to mention that there is some randomness in the process of building the decision tree with the related module from sklearn that could affect the results, that is is why a good approach would be to generate several trees and to classify according to the major vote (the class assigned will be the most reccurent one), but this is note what will be done here, for a simplification purpose.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> V.1) Decision tree and real-valued dataset with 35 variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with a tree of maximum size and real-valued dataset with 35 numerical variables: 0.67\n",
      "Score with optimal depth (obtained with a grid search): 0.71\n",
      "\n",
      "\n",
      "Optimal depth: 3\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[187  27]\n",
      " [ 60  26]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       214\n",
      "           1       0.49      0.30      0.37        86\n",
      "\n",
      "    accuracy                           0.71       300\n",
      "   macro avg       0.62      0.59      0.59       300\n",
      "weighted avg       0.68      0.71      0.69       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#numerical variables (35 components)\n",
    "clf = tree.DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_real_train, y_real_train)\n",
    "print(\"Score with a tree of maximum size and real-valued dataset with 35 numerical variables: \"+\\\n",
    "      str(clf.score(X_real_test, y_real_test)))\n",
    "\n",
    "\n",
    "depth= []\n",
    "for i in range(38):\n",
    "    depth.append(i+1)\n",
    "    \n",
    "pgrid = {\"max_depth\": depth}\n",
    "\n",
    "#looking for the best value for the max_depth parameter with 35 numerical variables \n",
    "\n",
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(random_state=1), param_grid=pgrid, cv=5, n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_real_train, y_real_train)\n",
    "\n",
    "\n",
    "print(\"Score with optimal depth (obtained with a grid search): \" + str(grid_search.best_estimator_.score(X_real_test, \\\n",
    "                                                                                                              y_real_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Optimal depth: \" + str(grid_search.best_estimator_.get_depth()))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = grid_search.predict(X_real_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_real_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_real_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>V.2) Decision tree and complete real-valued dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with a tree of maximum size and complete numerical variables: 0.6166666666666667\n",
      "Score with optimal depth (obtained with a grid search): 0.7033333333333334\n",
      "\n",
      "\n",
      "Optimal depth: 2\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[196  18]\n",
      " [ 71  15]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.81       214\n",
      "           1       0.45      0.17      0.25        86\n",
      "\n",
      "    accuracy                           0.70       300\n",
      "   macro avg       0.59      0.55      0.53       300\n",
      "weighted avg       0.65      0.70      0.65       300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "\n",
    "#numerical variables (76 components)\n",
    "clf = tree.DecisionTreeClassifier(random_state=2)\n",
    "clf.fit(X_real_complete_train, y_real_complete_train)\n",
    "print(\"Score with a tree of maximum size and complete numerical variables: \"+str(clf.score(X_real_complete_test, \\\n",
    "                                                                                            y_real_complete_test)))\n",
    "\n",
    "#looking for the best value for the max_depth parameter with the omplete numerical variables \n",
    "\n",
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(random_state=3), param_grid=pgrid, cv=5, n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_real_complete_train, y_real_complete_train)\n",
    "\n",
    "\n",
    "print(\"Score with optimal depth (obtained with a grid search): \" + str(grid_search.best_estimator_.score(\\\n",
    "                                                                   X_real_complete_test, y_real_complete_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Optimal depth: \" + str(grid_search.best_estimator_.get_depth()))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = grid_search.predict(X_real_complete_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_real_complete_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_real_complete_test, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>V.3) Decision tree and ordinal dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for a tree of maximum size and ordinal dataset: 0.6833333333333333\n",
      "Score with optimal depth (obtained with a grid search): 0.7366666666666667\n",
      "\n",
      "\n",
      "Optimal depth: 3\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[185  29]\n",
      " [ 50  36]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       214\n",
      "           1       0.55      0.42      0.48        86\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.67      0.64      0.65       300\n",
      "weighted avg       0.72      0.74      0.72       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#ordinal variables\n",
    "clf = tree.DecisionTreeClassifier(random_state=4)\n",
    "clf.fit(X_ordinal_train, y_ordinal_train)\n",
    "print(\"Score for a tree of maximum size and ordinal dataset: \"+str(clf.score(X_ordinal_test, y_ordinal_test)))\n",
    "\n",
    "#looking for the best value for the max_depth parameter with ordinal variables\n",
    "\n",
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(random_state=5), param_grid=pgrid, cv=5, n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_ordinal_train, y_ordinal_train)\n",
    "\n",
    "\n",
    "print(\"Score with optimal depth (obtained with a grid search): \" + str(grid_search.best_estimator_.score(X_ordinal_test, \\\n",
    "                                                                                                              y_ordinal_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Optimal depth: \" + str(grid_search.best_estimator_.get_depth()))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = grid_search.predict(X_ordinal_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_ordinal_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_ordinal_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> V.4) Decision tree and binary dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for a tree of maximum size and binary dataset: 0.6233333333333333\n",
      "Score with optimal depth (obtained with a grid search): 0.7433333333333333\n",
      "\n",
      "\n",
      "Optimal depth: 4\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[188  26]\n",
      " [ 51  35]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       214\n",
      "           1       0.57      0.41      0.48        86\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.68      0.64      0.65       300\n",
      "weighted avg       0.73      0.74      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#dummy variables\n",
    "clf = tree.DecisionTreeClassifier(random_state=6)\n",
    "clf.fit(X_dummy_train, y_dummy_train)\n",
    "print(\"Score for a tree of maximum size and binary dataset: \"+str(clf.score(X_dummy_test, y_dummy_test)))\n",
    "\n",
    "#looking for the best value for the max_depth parameter with dummy variables\n",
    "\n",
    "grid_search = GridSearchCV(tree.DecisionTreeClassifier(random_state=7), param_grid=pgrid, cv=5, n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_dummy_train, y_dummy_train)\n",
    "\n",
    "\n",
    "print(\"Score with optimal depth (obtained with a grid search): \" + str(grid_search.best_estimator_.score(X_dummy_test, \\\n",
    "                                                                                                              y_dummy_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Optimal depth: \" + str(grid_search.best_estimator_.get_depth()))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = grid_search.predict(X_dummy_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_dummy_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_dummy_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VI) SVM with kernel trick</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VI.1) SVM with kernel trick and real-valued dataset with 35 variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with an SVM with kernel trick and the real-valued dataset with 35 variables: 0.74\n",
      "\n",
      "\n",
      "Kernel and optimal parameters: {'C': 50, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[180  34]\n",
      " [ 44  42]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       214\n",
      "           1       0.55      0.49      0.52        86\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.68      0.66      0.67       300\n",
      "weighted avg       0.73      0.74      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#SVM with kernel trick\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#definition of different values for each parameter\n",
    "#of each kernel (rbf, polynomial or linear)\n",
    "#151 possible combinations\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['rbf'], 'gamma': ['auto', 'scale', 0.1, 1, 2, 5, 10, 12, 15, 17, 20], \\\n",
    "     'C': [0.01, 0.1, 1.0, 10, 20, 30, 50, 70, 100]},\n",
    "    {'kernel': ['poly'], 'degree': [3, 10, 30], 'C': [0.01, 0.1, 1.0, 5, 7, 10, 12, 15, 17, 20, 30, 50, 70, 100]},\n",
    "    {'kernel': ['linear'], 'C': [0.01, 0.1, 1.0, 10, 15, 20, 30, 50, 70, 100]}\n",
    "]\n",
    "\n",
    "#use of grid search to find the best parameters with 35 numerical variables\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(max_iter=100000), param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "clf.fit(X_real_train, y_real_train)\n",
    "\n",
    "print(\"Best score with an SVM with kernel trick and the real-valued dataset with 35 variables: \" + str(\\\n",
    "                                                                                        clf.best_estimator_.score(X_real_test,\\\n",
    "                                                                                        y_real_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Kernel and optimal parameters: \" + str(clf.best_params_))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = clf.best_estimator_.predict(X_real_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_real_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_real_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VI.2) SVM with kernel trick and complete real-valued dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with an SVM with kernel trick and the complete real-valued dataset: 0.76\n",
      "\n",
      "\n",
      "Kernel and optimal parameters: {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[193  21]\n",
      " [ 51  35]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84       214\n",
      "           1       0.62      0.41      0.49        86\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.71      0.65      0.67       300\n",
      "weighted avg       0.74      0.76      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#use of grid search to find the best parameters with the complete real variables:\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(max_iter=100000), param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "clf.fit(X_real_complete_train, y_real_complete_train)\n",
    "\n",
    "print(\"Best score with an SVM with kernel trick and the complete real-valued dataset: \" + str(clf.best_estimator_.score(\\\n",
    "                                                                                                    X_real_complete_test,\\\n",
    "                                                                                                    y_real_complete_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Kernel and optimal parameters: \" + str(clf.best_params_))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = clf.best_estimator_.predict(X_real_complete_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_real_complete_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_real_complete_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VI.3) SVM with kernel trick and ordinal dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with an SVM with kernel trick and the ordinal dataset: 0.77\n",
      "\n",
      "\n",
      "Kernel and optimal parameters: {'C': 1.0, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[189  25]\n",
      " [ 44  42]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85       214\n",
      "           1       0.63      0.49      0.55        86\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.72      0.69      0.70       300\n",
      "weighted avg       0.76      0.77      0.76       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#use of grid search to find the best parameters with ordinal variables:\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(max_iter=100000), param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "clf.fit(X_ordinal_train, y_ordinal_train)\n",
    "\n",
    "print(\"Best score with an SVM with kernel trick and the ordinal dataset: \" + str(clf.best_estimator_.score(X_ordinal_test,\\\n",
    "                                                                                                            y_ordinal_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Kernel and optimal parameters: \" + str(clf.best_params_))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = clf.best_estimator_.predict(X_ordinal_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_ordinal_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_ordinal_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VI.4) SVM with kernel trick and binary dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with an SVM with kernel trick and the binary dataset: 0.7433333333333333\n",
      "\n",
      "\n",
      "Kernel and optimal parameters: {'C': 20, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[179  35]\n",
      " [ 42  44]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       214\n",
      "           1       0.56      0.51      0.53        86\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.68      0.67      0.68       300\n",
      "weighted avg       0.74      0.74      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Abdelwahid Benslimane\n",
    "#wahid.benslimane@gmail.com\n",
    "\n",
    "#use of grid search to find the best parameters with ordinal variables:\n",
    "\n",
    "clf = GridSearchCV(svm.SVC(max_iter=100000), param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "clf.fit(X_dummy_train, y_dummy_train)\n",
    "\n",
    "print(\"Best score with an SVM with kernel trick and the binary dataset: \" + str(clf.best_estimator_.score(X_dummy_test,\\\n",
    "                                                                                                            y_dummy_test)))\n",
    "print(\"\\n\")\n",
    "print(\"Kernel and optimal parameters: \" + str(clf.best_params_))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_pred = clf.best_estimator_.predict(X_dummy_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_dummy_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_dummy_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>VII) Synthesis of the results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results turn out to be interesting. Indeed, the best result with a complete decision tree was obtained with the ordinal dataset. On the other hand, the best score obtained with an optimized (pruned) tree was with binary data. We can also observe that the optimal depth varies according to the dataset used. \n",
    "\n",
    "It is also interesting to note that the decision tree, complete or pruned, generalizes better when trained with the real-valued dataset containing only the 35 most important synthetic variables, carrying 77.66% of the total information, than with the complete real-valued dataset, but it is more ignificant with the complete tree.  \n",
    "\n",
    "For SVM with kernel trick, the best score was obtained with the ordinal dataset. The optimal hyper-parameters of the model also differ according to the dataset used. Unlike the decision tree, the SVM with kernel trick generalizes better when trained with the complete real-valued dataset, than with the real-valued dataset containing only the 35 most important synthetic variables.\n",
    "\n",
    "The precision is always greater when it comes to correctly classifying non-risky customers rather than risky customers, simply because the original dataset contains only 1000 individuals and risky customers are under-represented (30%).\n",
    "\n",
    "It is important to mention again that there is some randomness in the process of building the decision tree with the related module from sklearn that could affect the results, that is is why a good approach would be to generate several trees and to classify according to the major vote (the class assigned will be the most reccurent one). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
